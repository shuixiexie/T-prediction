{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from weighted_descriptors import compute_coupled_descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²ä¿å­˜åŠ æƒæ··åˆæè¿°ç¬¦ç»“æžœè‡³ï¼šexample_output.xlsx\n",
      "âŒ è®¡ç®—è¿‡ç¨‹ä¸­å‡ºé”™: [Errno 13] Permission denied: 'example_output.xlsx'\n",
      "           species endpoint                   effect  Duration_Value  \\\n",
      "0  Vibrio fischeri     EC50  luminescence inhibition            0.25   \n",
      "1  Vibrio fischeri     EC50  luminescence inhibition            0.25   \n",
      "2  Vibrio fischeri     EC50  luminescence inhibition            0.25   \n",
      "3  Vibrio fischeri     EC50  luminescence inhibition            0.25   \n",
      "4  Vibrio fischeri     EC50  luminescence inhibition            0.25   \n",
      "\n",
      "   ETA_Epsilon_2    GATS4e    GATS4c      TDB5s  naaaC  minHBint5  ...  \\\n",
      "0       0.856248  1.151021  0.973680  16.708067    0.0   0.000000  ...   \n",
      "1       0.855721  1.164841  0.979460  16.491798    0.0   0.000000  ...   \n",
      "2       0.858305  1.162362  0.981677  16.857363    0.0   0.019568  ...   \n",
      "3       0.955326  1.898476  1.039049   0.166630    0.0   0.000000  ...   \n",
      "4       0.955242  1.900696  1.039978   0.131878    0.0   0.000000  ...   \n",
      "\n",
      "     AATSC0v  RPSA_mix2  mindssC_mix2  minHsOH_mix2   MATS6c_mix2  \\\n",
      "0  48.240310   0.057346      0.005756           0.0  4.047420e-05   \n",
      "1  48.286547   0.056316      0.005756           0.0  2.129608e-06   \n",
      "2  48.149255   0.057319      0.005462           0.0  6.895380e-12   \n",
      "3  45.465725   0.248907      0.050757           0.0  1.640619e-05   \n",
      "4  45.473155   0.248560      0.050757           0.0  1.064496e-05   \n",
      "\n",
      "   MATS6p_mix2  minHCsats_mix2  FNSA-1_mix2  MATS5c_mix3  nHBint3_mix3  \n",
      "0     1.029351             0.0     0.185337     0.034177      0.042758  \n",
      "1     1.047043             0.0     0.184719     0.030514      0.000000  \n",
      "2     0.996580             0.0     0.194092     0.022479      0.000000  \n",
      "3     0.000007             0.0     0.126103     0.000723      0.006871  \n",
      "4     0.000001             0.0     0.126021     0.000134      0.000000  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# å‡è®¾å‡½æ•°å·²ç»åœ¨å½“å‰æ–‡ä»¶ä¸­æˆ–è€…å¯¼å…¥è¿‡\n",
    "# from weighted_descriptors import compute_coupled_descriptors\n",
    "\n",
    "# -------------------------------\n",
    "# 1ï¸âƒ£ è®¾ç½®æ–‡ä»¶è·¯å¾„\n",
    "# -------------------------------\n",
    "file_path = r\"data.xlsx\"\n",
    "output_path = r\"example_output.xlsx\"\n",
    "df_result = compute_coupled_descriptors(file_path, output_path=\"example_output.xlsx\")\n",
    "# -------------------------------\n",
    "# 2ï¸âƒ£ è°ƒç”¨å‡½æ•°è®¡ç®—æ··åˆæè¿°ç¬¦\n",
    "# -------------------------------\n",
    "try:\n",
    "    df_result = compute_coupled_descriptors(file_path, output_path=output_path)\n",
    "    print(\"âœ… æ··åˆæè¿°ç¬¦è®¡ç®—å®Œæˆï¼\")\n",
    "except ValueError as e:\n",
    "    print(f\"âŒ è¾“å…¥æ–‡ä»¶éªŒè¯å¤±è´¥: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è®¡ç®—è¿‡ç¨‹ä¸­å‡ºé”™: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3ï¸âƒ£ æŸ¥çœ‹ç»“æžœ\n",
    "# -------------------------------\n",
    "# æ‰“å°å‰äº”è¡Œ\n",
    "print(df_result.head())\n",
    "\n",
    "# å¦‚æžœæ²¡æœ‰æä¾› output_pathï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨ä¿å­˜\n",
    "if output_path is None:\n",
    "    df_result.to_excel(\"weighted_descriptors_result.xlsx\", index=False)\n",
    "    print(\"å·²æ‰‹åŠ¨ä¿å­˜ç»“æžœåˆ° weighted_descriptors_result.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.é¢„æµ‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ä¿å­˜æ ‡å‡†åŒ–å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ ‡å‡†åŒ–åŽçš„æ•°æ®å·²ä¿å­˜åˆ° data01_normalized.xlsx\n",
      "âœ… æ¯ä¸ªåˆ†ç»„çš„æ ‡å‡†åŒ–å™¨å·²ä¿å­˜åˆ° scalers_for_groups.save\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "def normalize_by_group(\n",
    "    data_path, sheet_name, output_path=None, scaler_save_path=\"scalers_for_groups.save\"\n",
    "):\n",
    "    \"\"\"\n",
    "    å¯¹æŒ‡å®šæ´»åŠ¨è¡¨çš„æ•°æ®æŒ‰åˆ†ç»„è¿›è¡Œ Z-score æ ‡å‡†åŒ–ï¼Œå¹¶ä¿å­˜æ ‡å‡†åŒ–å™¨ã€‚\n",
    "    æ¯ä¸ªåˆ†ç»„ç”± (\"species\", \"endpoint\", \"effect\", \"Duration_Value\") å”¯ä¸€ç¡®å®šã€‚\n",
    "\n",
    "    :param data_path: è¾“å…¥ Excel æ–‡ä»¶è·¯å¾„\n",
    "    :param sheet_name: è¦å¤„ç†çš„æ´»åŠ¨è¡¨åç§°\n",
    "    :param output_path: å¯é€‰ï¼Œä¿å­˜æ ‡å‡†åŒ–åŽçš„å®Œæ•´æ•°æ®è·¯å¾„\n",
    "    :param scaler_save_path: ä¿å­˜æ¯ä¸ªåˆ†ç»„ StandardScaler çš„è·¯å¾„\n",
    "    :return: æ ‡å‡†åŒ–åŽçš„ DataFrame\n",
    "    \"\"\"\n",
    "    # è¯»å– Excel\n",
    "    df = pd.read_excel(data_path, sheet_name=sheet_name)\n",
    "\n",
    "    # åˆ†ç»„\n",
    "    grouped = df.groupby([\"species\", \"endpoint\", \"effect\", \"Duration_Value\"])\n",
    "    scaler_dict = {}\n",
    "    normalized_list = []\n",
    "\n",
    "    for name, group in grouped:\n",
    "        # ç‰¹å¾åˆ—å‡è®¾ä»Žç¬¬15åˆ—å¼€å§‹\n",
    "        features = group.iloc[:, 14:]\n",
    "\n",
    "        # Z-score æ ‡å‡†åŒ–\n",
    "        scaler = StandardScaler()\n",
    "        features_normalized = scaler.fit_transform(features)\n",
    "        group.iloc[:, 14:] = features_normalized\n",
    "\n",
    "        # ä¿å­˜ scaler\n",
    "        scaler_dict[name] = scaler\n",
    "\n",
    "        normalized_list.append(group)\n",
    "\n",
    "    # åˆå¹¶æ‰€æœ‰åˆ†ç»„\n",
    "    df_normalized = pd.concat(normalized_list, axis=0)\n",
    "\n",
    "    # ä¿å­˜æ ‡å‡†åŒ–åŽçš„å®Œæ•´æ•°æ®ï¼ˆå¯é€‰ï¼‰\n",
    "    if output_path:\n",
    "        df_normalized.to_excel(output_path, index=False)\n",
    "        print(f\"âœ… æ ‡å‡†åŒ–åŽçš„æ•°æ®å·²ä¿å­˜åˆ° {output_path}\")\n",
    "\n",
    "    # ä¿å­˜æ‰€æœ‰åˆ†ç»„çš„ StandardScaler\n",
    "    joblib.dump(scaler_dict, scaler_save_path)\n",
    "    print(f\"âœ… æ¯ä¸ªåˆ†ç»„çš„æ ‡å‡†åŒ–å™¨å·²ä¿å­˜åˆ° {scaler_save_path}\")\n",
    "\n",
    "    return df_normalized\n",
    "\n",
    "# -------------------------\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "# -------------------------\n",
    "data_path = \"data01.xlsx\"\n",
    "sheet_name = \"Combined Results\"\n",
    "output_path = \"data01_normalized.xlsx\"\n",
    "\n",
    "df_norm = normalize_by_group(\n",
    "    data_path=data_path,\n",
    "    sheet_name=sheet_name,\n",
    "    output_path=output_path,\n",
    "    scaler_save_path=\"scalers_for_groups.save\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 æ¨¡åž‹å®šä¹‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TransformerMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, transformer_layers, num_heads, ff_dim, output_dim):\n",
    "        super(TransformerMLP, self).__init__()\n",
    "        if input_dim % num_heads != 0:\n",
    "            for i in range(num_heads, 0, -1):\n",
    "                if input_dim % i == 0:\n",
    "                    num_heads = i\n",
    "                    break\n",
    "            print(f\"Adjusted num_heads to {num_heads} for compatibility with input_dim {input_dim}\")\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=transformer_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path, train_sheet, test_sheet, target_column):\n",
    "    # è¯»å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    train_df = pd.read_excel(file_path, sheet_name=train_sheet)\n",
    "    test_df = pd.read_excel(file_path, sheet_name=test_sheet)\n",
    "\n",
    "    # åˆ†ç¦»ç‰¹å¾ä¸Žç›®æ ‡\n",
    "    X_train = train_df.drop(columns=[target_column])\n",
    "    y_train = train_df[target_column]\n",
    "    X_test = test_df.drop(columns=[target_column])\n",
    "    y_test = test_df[target_column]\n",
    "\n",
    "    # åˆå¹¶å†åš one-hot ç¼–ç ï¼ˆä¿æŒè®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾ä¸€è‡´ï¼‰\n",
    "    combined = pd.concat([X_train, X_test], axis=0)\n",
    "    combined_encoded = pd.get_dummies(combined)\n",
    "\n",
    "    # æ‹†åˆ†å›žè®­ç»ƒé›†ä¸Žæµ‹è¯•é›†\n",
    "    X_train_encoded = combined_encoded.iloc[:X_train.shape[0], :]\n",
    "    X_test_encoded = combined_encoded.iloc[X_train.shape[0]:, :]\n",
    "\n",
    "    # ðŸš« ä¸å†ä½¿ç”¨ StandardScaler()\n",
    "    # âœ… å› ä¸º Excel æ•°æ®å·²ç»æ˜¯ Z-score æ ‡å‡†åŒ–ç»“æžœ\n",
    "\n",
    "    # è½¬æ¢ä¸º tensor\n",
    "    return (\n",
    "        torch.tensor(X_train_encoded.values, dtype=torch.float32),\n",
    "        torch.tensor(y_train.values, dtype=torch.float32),\n",
    "        torch.tensor(X_test_encoded.values, dtype=torch.float32),\n",
    "        torch.tensor(y_test.values, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def train_transformer_mlp_model(X_train, y_train, input_dim, epochs=100, batch_size=32, learning_rate=1e-3):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TransformerMLP(input_dim, hidden_dim=100, transformer_layers=5, num_heads=5, ff_dim=256, output_dim=1)\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    dataset = TensorDataset(X_train, y_train.unsqueeze(1))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    train_predictions = []\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "        epoch_predictions = []\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_predictions.extend(y_pred.cpu().detach().numpy().flatten())\n",
    "        r2 = r2_score(y_train.numpy(), epoch_predictions)\n",
    "        if r2 > best_score:\n",
    "            best_score = r2\n",
    "            best_model = model.state_dict()\n",
    "        train_predictions = epoch_predictions\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, train_predictions\n",
    "\n",
    "def evaluate_model(model, X, y, dataset_name, output_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X.to(device)).cpu().numpy().flatten()\n",
    "    r2 = r2_score(y.numpy(), predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y.numpy(), predictions))\n",
    "    mae = mean_absolute_error(y.numpy(), predictions)\n",
    "    print(f\"{dataset_name} Set Evaluation:\")\n",
    "    print(f\"  R2 Score: {r2:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Actual\": y.numpy(),\n",
    "        \"Predicted\": predictions\n",
    "    })\n",
    "    results_df.to_excel(output_path, index=False)\n",
    "    return {\"R2\": r2, \"RMSE\": rmse, \"MAE\": mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted num_heads to 1 for compatibility with input_dim 37\n",
      "Adjusted num_heads to 1 for compatibility with input_dim 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:50<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡åž‹æƒé‡å·²ä¿å­˜\n",
      "âœ… ç‰¹å¾åˆ—é¡ºåºå·²ä¿å­˜\n",
      "è®­ç»ƒé›†è¯„ä¼°ç»“æžœ:\n",
      "  R2: 0.9679\n",
      "  RMSE: 0.3533\n",
      "  MAE: 0.2285\n",
      "âœ… è®­ç»ƒé›†é¢„æµ‹ç»“æžœå·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler  # ä»éœ€å¯¼å…¥ï¼Œä¸ºä¿æŒå…¼å®¹æ€§\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# 1. è¯»å–è®­ç»ƒé›† Excel\n",
    "file_path = r\"F:\\T-prediction\\data_train.xlsx\"  # æ›¿æ¢ä¸ºä½ çš„è®­ç»ƒé›†è·¯å¾„\n",
    "target_column = \"pEC50M (Mol/L)\"  # æ›¿æ¢ä¸ºä½  Excel ä¸­çš„ç›®æ ‡åˆ—å\n",
    "train_df = pd.read_excel(file_path)\n",
    "\n",
    "# 2. å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡\n",
    "X_train = train_df.drop(columns=[target_column])\n",
    "y_train = train_df[target_column]\n",
    "\n",
    "# ä»…å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œ one-hot ç¼–ç ï¼ˆä¸å†æ ‡å‡†åŒ–ï¼‰\n",
    "categorical_cols = [\"species\", \"endpoint\", \"effect\", \"Duration_Value\"]\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)\n",
    "\n",
    "# ðŸš« ä¸è¿›è¡Œ Z æ ‡å‡†åŒ–ï¼Œå› ä¸ºæ•°æ®å·²ç»æ˜¯æ ‡å‡†åŒ–åŽçš„\n",
    "# scaler = StandardScaler()\n",
    "# X_train_encoded[X_train_encoded.columns] = scaler.fit_transform(X_train_encoded)\n",
    "\n",
    "# è½¬æ¢ä¸º tensor\n",
    "X_train_tensor = torch.tensor(X_train_encoded.values.astype(np.float32))\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32))\n",
    "\n",
    "\n",
    "# åˆ›å»ºæ¨¡åž‹\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = TransformerMLP(input_dim, hidden_dim=100, transformer_layers=5, num_heads=5, ff_dim=256, output_dim=1)\n",
    "\n",
    "# è®­ç»ƒæ¨¡åž‹\n",
    "model, train_predictions = train_transformer_mlp_model(X_train_tensor, y_train_tensor, input_dim)\n",
    "\n",
    "# ä¿å­˜æ¨¡åž‹æƒé‡\n",
    "torch.save(model.state_dict(), \"Transformer_DNN_model.pth\")\n",
    "print(\"âœ… æ¨¡åž‹æƒé‡å·²ä¿å­˜\")\n",
    "\n",
    "# --------------------------\n",
    "# ä¿å­˜ one-hot ç¼–ç åˆ—é¡ºåº\n",
    "# --------------------------\n",
    "feature_columns = X_train_encoded.columns.tolist()\n",
    "joblib.dump(feature_columns, \"feature_columns.save\")\n",
    "print(\"âœ… ç‰¹å¾åˆ—é¡ºåºå·²ä¿å­˜\")\n",
    "\n",
    "# --------------------------\n",
    "# å› ä¸ºæ²¡æœ‰ä½¿ç”¨æ ‡å‡†åŒ–å™¨ï¼Œæ‰€ä»¥ä¸ä¿å­˜ scaler\n",
    "# --------------------------\n",
    "# joblib.dump(scaler, \"scaler.save\")\n",
    "\n",
    "# --------------------------\n",
    "# è®­ç»ƒé›†è¯„ä¼°\n",
    "# --------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_train_tensor).cpu().numpy().flatten()\n",
    "\n",
    "r2 = r2_score(y_train_tensor.numpy(), y_pred)\n",
    "rmse = mean_squared_error(y_train_tensor.numpy(), y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_train_tensor.numpy(), y_pred)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†è¯„ä¼°ç»“æžœ:\")\n",
    "print(f\"  R2: {r2:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  MAE: {mae:.4f}\")\n",
    "\n",
    "# å¯é€‰ï¼šä¿å­˜è®­ç»ƒé›†é¢„æµ‹ç»“æžœåˆ° Excel\n",
    "results_df = pd.DataFrame({\"Actual\": y_train_tensor.numpy(), \"Predicted\": y_pred})\n",
    "results_df.to_excel(\"train_predictions.xlsx\", index=False)\n",
    "print(\"âœ… è®­ç»ƒé›†é¢„æµ‹ç»“æžœå·²ä¿å­˜\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "from transformer_model import TransformerMLP  # æ›¿æ¢ä¸ºä½ ä¿å­˜æ¨¡åž‹ç±»çš„æ–‡ä»¶å\n",
    "\n",
    "st.set_page_config(page_title=\"T-prediction\", layout=\"wide\")\n",
    "st.title(\"T-prediction: é¢„æµ‹å·¥å…·ï¼ˆæµ‹è¯•ç‰ˆï¼‰\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"ä¸Šä¼ å¾…é¢„æµ‹çš„ Excel æ–‡ä»¶\", type=[\"xlsx\"])\n",
    "\n",
    "if uploaded_file:\n",
    "    try:\n",
    "        st.info(\"æ­£åœ¨è¯»å–æ–‡ä»¶å¹¶è¿›è¡Œæ•°æ®é¢„å¤„ç†...\")\n",
    "        predict_df = pd.read_excel(uploaded_file)\n",
    "\n",
    "        # è¯»å–ç‰¹å¾åˆ—å’Œæ ‡å‡†åŒ–å™¨\n",
    "        feature_columns = joblib.load(\"feature_columns.save\")\n",
    "        scaler = joblib.load(\"scaler.save\")\n",
    "\n",
    "        # one-hot ç¼–ç \n",
    "        categorical_cols = [\"species\", \"endpoint\", \"effect\", \"Duration_Value\"]\n",
    "        predict_encoded = pd.get_dummies(predict_df, columns=categorical_cols)\n",
    "\n",
    "        # å¯¹é½åˆ—é¡ºåº\n",
    "        for col in feature_columns:\n",
    "            if col not in predict_encoded.columns:\n",
    "                predict_encoded[col] = 0\n",
    "        predict_encoded = predict_encoded[feature_columns]\n",
    "\n",
    "        # æ ‡å‡†åŒ–\n",
    "        predict_scaled = scaler.transform(predict_encoded)\n",
    "        X_predict = torch.tensor(predict_scaled, dtype=torch.float32)\n",
    "\n",
    "        # åŠ è½½æ¨¡åž‹\n",
    "        input_dim = X_predict.shape[1]\n",
    "        model = TransformerMLP(input_dim, hidden_dim=100, transformer_layers=5, num_heads=5, ff_dim=256, output_dim=1)\n",
    "        model.load_state_dict(torch.load(\"Transformer_DNN_model.pth\", map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "\n",
    "        # é¢„æµ‹\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_predict).numpy().flatten()\n",
    "\n",
    "        # æ˜¾ç¤º\n",
    "        predict_df[\"Predicted\"] = predictions\n",
    "        st.success(\"é¢„æµ‹å®Œæˆï¼\")\n",
    "        st.dataframe(predict_df)\n",
    "\n",
    "        # ä¸‹è½½\n",
    "        output_file = \"prediction_results.xlsx\"\n",
    "        predict_df.to_excel(output_file, index=False)\n",
    "        st.download_button(\n",
    "            label=\"ä¸‹è½½é¢„æµ‹ç»“æžœ\",\n",
    "            data=open(output_file, \"rb\"),\n",
    "            file_name=\"prediction_results.xlsx\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"é¢„æµ‹å¤±è´¥: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
